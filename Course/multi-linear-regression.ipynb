{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "b4474bd39aa979d9034cf80f10e2f11f4a0fc3802d205559d129ca421827463e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Multi linear Regession\r\n",
    "# Assumptions\r\n",
    "# Linearity\r\n",
    "# Homoscedasticity\r\n",
    "# Multivariate normality\r\n",
    "# Independence of errors\r\n",
    "# Lack of multicollinearity\r\n",
    "# y = constant (b0) + b1X1 + b2X2 + b3X3 + ???\r\n",
    "# In this model, our State is the dummy variable\r\n",
    "\r\n",
    "# Building a model ( Backward Elimination)\r\n",
    "# 1) select significance level to stay in the model (eg SL=0.005)\r\n",
    "# 2) Fit the model wit all possible predictors\r\n",
    "# 3) Consider the predictor with the highest P-value. If P > SL, go to Step 4, otherwise go to Fin\r\n",
    "# 4) Remove the predictor\r\n",
    "# 5) Fit models without this variable\r\n",
    "\r\n",
    "# Building a model ( Forward Elimination)\r\n",
    "# 1) select significance level to enter in the model (eg SL=0.005)\r\n",
    "# 2) Fit all simple regression models y~xn Select the one with the lowest P-value\r\n",
    "# 3) Keep this variable and fit all possible models with one extra predictor added to the ones you already have\r\n",
    "# 4) Consider the predictor with the lowest P-value. If P < SL, go to Step three, otherwise go to Fin\r\n",
    "# 5) Fit models without this variable\r\n",
    "\r\n",
    "# Building a model ( Bidirectional Elimination)\r\n",
    "# 1) select a significance level to enter or stay in the model (eg SLenter=0.05, SLstay=0.05)\r\n",
    "# 2) Perform the next step of forward selection (new variables ,ust have: P < SLenter to enter)\r\n",
    "# 3) Perform all steps of backward elimination (old variables ,ust have: P < SLstay to stay)\r\n",
    "# 4) No new variable can enter and no old variable can exit\r\n",
    "\r\n",
    "import numpy as np \r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Dependent and Independent variables\r\n",
    "\r\n",
    "dataset = pd.read_csv('data/50_Startups.csv')\r\n",
    "x = dataset.iloc[:, :-1].values # : means the range and -1 means less the last column\r\n",
    "y = dataset.iloc[:,-1].values\r\n",
    "# y = dataset['Profit'].values\r\n",
    "print(dataset)\r\n",
    "# print(x)\r\n",
    "# print(y)\r\n",
    "# print(x.shape)\r\n",
    "# print(y.shape)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    R&D Spend  Administration  Marketing Spend       State     Profit\n",
      "0   165349.20       136897.80        471784.10    New York  192261.83\n",
      "1   162597.70       151377.59        443898.53  California  191792.06\n",
      "2   153441.51       101145.55        407934.54     Florida  191050.39\n",
      "3   144372.41       118671.85        383199.62    New York  182901.99\n",
      "4   142107.34        91391.77        366168.42     Florida  166187.94\n",
      "5   131876.90        99814.71        362861.36    New York  156991.12\n",
      "6   134615.46       147198.87        127716.82  California  156122.51\n",
      "7   130298.13       145530.06        323876.68     Florida  155752.60\n",
      "8   120542.52       148718.95        311613.29    New York  152211.77\n",
      "9   123334.88       108679.17        304981.62  California  149759.96\n",
      "10  101913.08       110594.11        229160.95     Florida  146121.95\n",
      "11  100671.96        91790.61        249744.55  California  144259.40\n",
      "12   93863.75       127320.38        249839.44     Florida  141585.52\n",
      "13   91992.39       135495.07        252664.93  California  134307.35\n",
      "14  119943.24       156547.42        256512.92     Florida  132602.65\n",
      "15  114523.61       122616.84        261776.23    New York  129917.04\n",
      "16   78013.11       121597.55        264346.06  California  126992.93\n",
      "17   94657.16       145077.58        282574.31    New York  125370.37\n",
      "18   91749.16       114175.79        294919.57     Florida  124266.90\n",
      "19   86419.70       153514.11             0.00    New York  122776.86\n",
      "20   76253.86       113867.30        298664.47  California  118474.03\n",
      "21   78389.47       153773.43        299737.29    New York  111313.02\n",
      "22   73994.56       122782.75        303319.26     Florida  110352.25\n",
      "23   67532.53       105751.03        304768.73     Florida  108733.99\n",
      "24   77044.01        99281.34        140574.81    New York  108552.04\n",
      "25   64664.71       139553.16        137962.62  California  107404.34\n",
      "26   75328.87       144135.98        134050.07     Florida  105733.54\n",
      "27   72107.60       127864.55        353183.81    New York  105008.31\n",
      "28   66051.52       182645.56        118148.20     Florida  103282.38\n",
      "29   65605.48       153032.06        107138.38    New York  101004.64\n",
      "30   61994.48       115641.28         91131.24     Florida   99937.59\n",
      "31   61136.38       152701.92         88218.23    New York   97483.56\n",
      "32   63408.86       129219.61         46085.25  California   97427.84\n",
      "33   55493.95       103057.49        214634.81     Florida   96778.92\n",
      "34   46426.07       157693.92        210797.67  California   96712.80\n",
      "35   46014.02        85047.44        205517.64    New York   96479.51\n",
      "36   28663.76       127056.21        201126.82     Florida   90708.19\n",
      "37   44069.95        51283.14        197029.42  California   89949.14\n",
      "38   20229.59        65947.93        185265.10    New York   81229.06\n",
      "39   38558.51        82982.09        174999.30  California   81005.76\n",
      "40   28754.33       118546.05        172795.67  California   78239.91\n",
      "41   27892.92        84710.77        164470.71     Florida   77798.83\n",
      "42   23640.93        96189.63        148001.11  California   71498.49\n",
      "43   15505.73       127382.30         35534.17    New York   69758.98\n",
      "44   22177.74       154806.14         28334.72  California   65200.33\n",
      "45    1000.23       124153.04          1903.93    New York   64926.08\n",
      "46    1315.46       115816.21        297114.46     Florida   49490.75\n",
      "47       0.00       135426.92             0.00  California   42559.73\n",
      "48     542.05        51743.15             0.00    New York   35673.41\n",
      "49       0.00       116983.80         45173.06  California   14681.40\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Check dataframe for missing values\r\n",
    "\r\n",
    "# dataset.fillna(0, inplace=True) # This is to fill out 0 for all missing values\r\n",
    "\r\n",
    "missing = dataset.isnull().sum()\r\n",
    "print(missing)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R&D Spend          0\n",
      "Administration     0\n",
      "Marketing Spend    0\n",
      "State              0\n",
      "Profit             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Encoding the independent variables\r\n",
    "\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "\r\n",
    "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[3])],remainder='passthrough') # Pass through will not encode the other columns. [3] represents index to encode\r\n",
    "x = np.array(ct.fit_transform(x))\r\n",
    "print(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Splitting data into Training and Test set\r\n",
    "# To measure if the model is good enough, we can use a method called Train/Test.\r\n",
    "# Train/Test is a method to measure the accuracy of your model.\r\n",
    "# It is called Train/Test because you split the the data set into two sets: a training set and a testing set.\r\n",
    "# 80% for training, and 20% for testing.\r\n",
    "# You train the model using the training set.\r\n",
    "# You test the model using the testing set.\r\n",
    "# Train the model means create the model.\r\n",
    "# Test the model means test the accuracy of the model.\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=1)\r\n",
    "\r\n",
    "print(x_train)\r\n",
    "#print(x_test)\r\n",
    "print(y_train)\r\n",
    "#print(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Trainig the multi Linear regresion model on the training set\r\n",
    "# Dummy variable trap ???\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "regressor = LinearRegression()\r\n",
    "regressor.fit(x_train,y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Predict the Test set results\r\n",
    "# Here we are analysing the predicted vs the actual test data\r\n",
    "# Our results show the predicted vs the real profits\r\n",
    "\r\n",
    "y_pred = regressor.predict(x_test) # predicted profits using data from the test set\r\n",
    "np.set_printoptions(precision=2)\r\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.reshape(len(y_test),1)),1)) # Just 1 column. 0 = vertical axis and 1 = horizontal axis\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualising training test results\r\n",
    "\r\n",
    "plt.scatter(x_train[:,0],y_train, color = 'red')\r\n",
    "plt.plot(x_train,regressor.predict(x_train),color = 'blue')\r\n",
    "plt.title('Expense vs Profit (Training set)')\r\n",
    "plt.xlabel('Expected profit')\r\n",
    "plt.ylabel('Expected cost')\r\n",
    "plt.show() "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualising training train results\r\n",
    "\r\n",
    "plt.scatter(x_test[:,0],y_test, color = 'red')\r\n",
    "plt.plot(x_train,regressor.predict(x_train),color = 'blue')\r\n",
    "plt.title('Expense vs Profit (T est set)')\r\n",
    "plt.xlabel('Expected profit')\r\n",
    "plt.ylabel('Expected cost')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "from sklearn import linear_model\r\n",
    "\r\n",
    "ols = linear_model.LinearRegression()\r\n",
    "model = ols.fit(x, y)\r\n",
    "model.coef_ # The linear regression coefficient can be accessed in a form of class attribute with model.coef_\r\n",
    "model.intercept_ # The y-intercept can be accessed in a form of class attribute with model.intercept_\r\n",
    "model.score(x, y) # How good was your model? You can evaluate your model performance in a form of R-squared, with model.score(X, y). X is the features, and y is the response variable used to fit the model.\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}