{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "b4474bd39aa979d9034cf80f10e2f11f4a0fc3802d205559d129ca421827463e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np \r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd \r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating a dataset from data frame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Features or independent variables are those used in predicting results\r\n",
    "# Dependent variable is the result of the prediction in the last column\r\n",
    "# iloc is used in locating indexes. The range includes the lower bound and excludes the upper bound\r\n",
    "\r\n",
    "dataset = pd.read_csv('..\\data\\Data.csv')\r\n",
    "x = dataset.iloc[:, :-1].values # ['Take all rows','Take all columns except last column']\r\n",
    "y = dataset.iloc[:,-1].values # ['Take all rows','Take onlylast column']\r\n",
    "# print(dataset)\r\n",
    "print(x)\r\n",
    "print(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Handling missing data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We can replace missing values with the mean, median or most used variable\r\n",
    "# Transform returns the columns with the replacement done. To update the replacement, we assign transform to x \r\n",
    "\r\n",
    "imputer = SimpleImputer(missing_values=np.nan,strategy='mean') # replace all missing values represented by na with the mean\r\n",
    "imputer.fit(x[:, 1:3]) # Fit expects only columns with numerical values. The upper bound in python is excluded thus we do not say 1:2. This will not take into consideration the salary column\r\n",
    "x[:, 1:3] = imputer.transform(x[:, 1:3]) # Returns the new updated version of x\r\n",
    "print(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Encoding categorical data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Categorical data is simply information aggregated into groups rather than being in numeric formats, such as Gender, Sex or \r\n",
    "# Education Level. They are present in almost all real-life datasets, yet the current algorithms still struggle to deal with them.\r\n",
    "# A one hot encoding is a representation of categorical variables as binary vectors. This first requires that the categorical values \r\n",
    "# be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, \r\n",
    "# which is marked with a 1.\r\n",
    "\r\n",
    "\r\n",
    "# Encoding the independent variable\r\n",
    "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])],remainder='passthrough') # Pass through will not encode the other columns\r\n",
    "x = np.array(ct.fit_transform(x)) # Convert the transformd vector into a numpy array\r\n",
    "print(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Encoding the dependent variable\r\n",
    "# We just need to encode the dependent variable to 0 and 1 using the label encoder class\r\n",
    "\r\n",
    "le = LabelEncoder()\r\n",
    "y = le.fit_transform(y)\r\n",
    "print(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Splitting data into Training and Test set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The need for quality, accurate, complete, and relevant data starts early on in the training process. Only if the algorithm is fed \r\n",
    "# with good training data can it easily pick up the features and find relationships that it needs to predict down the line.\r\n",
    "# The machine learning models thus expect this format as inputs\r\n",
    "\r\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=1)\r\n",
    "\r\n",
    "print(x_train)\r\n",
    "print(x_test)\r\n",
    "print(y_train)\r\n",
    "print(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feature scaling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Feature scaling should be applied after the split\r\n",
    "# Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, \r\n",
    "# it is also known as data normalization and is generally performed during the data preprocessing step. Just to give you an \r\n",
    "# example — if you have multiple independent variables like age, salary, and height; With their range as (18–100 Years), \r\n",
    "# (25,000–75,000 Euros), and (1–2 Meters) respectively, feature scaling would help them all to be in the same range, for \r\n",
    "# example- centered around 0 or in the range (0,1) depending on the scaling technique.\r\n",
    "# https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35\r\n",
    "\r\n",
    "# Standardisation and Normalisation are two techniques used in scalling data\r\n",
    "\r\n",
    "\r\n",
    "sc = StandardScaler()\r\n",
    "# We do not feature scale dummy variables\r\n",
    "x_train[:, 3:] = sc.fit_transform(x_train[:, 3:])   # Exclude dummy colums thus take from third index\r\n",
    "                                                    # ['Take all rows','Take from 3rd column onwards']\r\n",
    "x_test[:, 3:] = sc.transform(x_test[:, 3:]) # We need same scaler thus we apply the transform method\r\n",
    "print(x_train)\r\n",
    "print(x_test)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "  Steps to follow\r\n",
    "- Create a dataset from data frame\r\n",
    "- Handle missing data\r\n",
    "- Encode categorical data\r\n",
    "- Split data into Training and Test set\r\n",
    "- Feature scaling"
   ],
   "metadata": {}
  }
 ]
}