{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "b4474bd39aa979d9034cf80f10e2f11f4a0fc3802d205559d129ca421827463e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Multi linear Regession\r\n",
    "# Assumptions\r\n",
    "# Linearity\r\n",
    "# Homoscedasticity\r\n",
    "# Multivariate normality\r\n",
    "# Independence of errors\r\n",
    "# Lack of multicollinearity\r\n",
    "# y = constant (b0) + b1X1 + b2X2 + b3X3 + ???\r\n",
    "# In this model, our State is the dummy variable\r\n",
    "\r\n",
    "# Building a model ( Backward Elimination)\r\n",
    "# 1) select significance level to stay in the model (eg SL=0.005)\r\n",
    "# 2) Fit the model wit all possible predictors\r\n",
    "# 3) Consider the predictor with the highest P-value. If P > SL, go to Step 4, otherwise go to Fin\r\n",
    "# 4) Remove the predictor\r\n",
    "# 5) Fit models without this variable\r\n",
    "\r\n",
    "# Building a model ( Forward Elimination)\r\n",
    "# 1) select significance level to enter in the model (eg SL=0.005)\r\n",
    "# 2) Fit all simple regression models y~xn Select the one with the lowest P-value\r\n",
    "# 3) Keep this variable and fit all possible models with one extra predictor added to the ones you already have\r\n",
    "# 4) Consider the predictor with the lowest P-value. If P < SL, go to Step three, otherwise go to Fin\r\n",
    "# 5) Fit models without this variable\r\n",
    "\r\n",
    "# Building a model ( Bidirectional Elimination)\r\n",
    "# 1) select a significance level to enter or stay in the model (eg SLenter=0.05, SLstay=0.05)\r\n",
    "# 2) Perform the next step of forward selection (new variables ,ust have: P < SLenter to enter)\r\n",
    "# 3) Perform all steps of backward elimination (old variables ,ust have: P < SLstay to stay)\r\n",
    "# 4) No new variable can enter and no old variable can exit\r\n",
    "\r\n",
    "import numpy as np \r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Dependent and Independent variables\r\n",
    "\r\n",
    "dataset = pd.read_csv('data/50_Startups.csv')\r\n",
    "x = dataset.iloc[:, :-1].values # : means the range and -1 means less the last column\r\n",
    "y = dataset.iloc[:,-1].values\r\n",
    "# y = dataset['Profit'].values\r\n",
    "print(dataset)\r\n",
    "# print(x)\r\n",
    "# print(y)\r\n",
    "# print(x.shape)\r\n",
    "# print(y.shape)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check dataframe for missing values\r\n",
    "\r\n",
    "# dataset.fillna(0, inplace=True) # This is to fill out 0 for all missing values\r\n",
    "\r\n",
    "missing = dataset.isnull().sum()\r\n",
    "print(missing)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Encoding the independent variables\r\n",
    "\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "\r\n",
    "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[3])],remainder='passthrough') # Pass through will not encode the other columns. [3] represents index to encode\r\n",
    "x = np.array(ct.fit_transform(x))\r\n",
    "print(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Splitting data into Training and Test set\r\n",
    "# To measure if the model is good enough, we can use a method called Train/Test.\r\n",
    "# Train/Test is a method to measure the accuracy of your model.\r\n",
    "# It is called Train/Test because you split the the data set into two sets: a training set and a testing set.\r\n",
    "# 80% for training, and 20% for testing.\r\n",
    "# You train the model using the training set.\r\n",
    "# You test the model using the testing set.\r\n",
    "# Train the model means create the model.\r\n",
    "# Test the model means test the accuracy of the model.\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=1)\r\n",
    "\r\n",
    "print(x_train)\r\n",
    "#print(x_test)\r\n",
    "print(y_train)\r\n",
    "#print(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Trainig the multi Linear regresion model on the training set\r\n",
    "# Dummy variable trap ???\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "regressor = LinearRegression()\r\n",
    "regressor.fit(x_train,y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Predict the Test set results\r\n",
    "# Here we are analysing the predicted vs the actual test data\r\n",
    "# Our results show the predicted vs the real profits\r\n",
    "\r\n",
    "y_pred = regressor.predict(x_test) # predicted profits using data from the test set\r\n",
    "np.set_printoptions(precision=2)\r\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.reshape(len(y_test),1)),1)) # Just 1 column. 0 = vertical axis and 1 = horizontal axis\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualising training test results\r\n",
    "\r\n",
    "plt.scatter(x_train[:,0],y_train, color = 'red')\r\n",
    "plt.plot(x_train,regressor.predict(x_train),color = 'blue')\r\n",
    "plt.title('Expense vs Profit (Training set)')\r\n",
    "plt.xlabel('Expected profit')\r\n",
    "plt.ylabel('Expected cost')\r\n",
    "plt.show() "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualising training train results\r\n",
    "\r\n",
    "plt.scatter(x_test[:,0],y_test, color = 'red')\r\n",
    "plt.plot(x_train,regressor.predict(x_train),color = 'blue')\r\n",
    "plt.title('Expense vs Profit (T est set)')\r\n",
    "plt.xlabel('Expected profit')\r\n",
    "plt.ylabel('Expected cost')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "from sklearn import linear_model\r\n",
    "\r\n",
    "ols = linear_model.LinearRegression()\r\n",
    "model = ols.fit(x, y)\r\n",
    "model.coef_ # The linear regression coefficient can be accessed in a form of class attribute with model.coef_\r\n",
    "model.intercept_ # The y-intercept can be accessed in a form of class attribute with model.intercept_\r\n",
    "model.score(x, y) # How good was your model? You can evaluate your model performance in a form of R-squared, with model.score(X, y). X is the features, and y is the response variable used to fit the model.\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}