{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "b4474bd39aa979d9034cf80f10e2f11f4a0fc3802d205559d129ca421827463e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np \r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Creating a dataset from data frame\r\n",
    "# Features or independent variables are those used in predicting results\r\n",
    "# Dependent variable is the result of the prediction in the last column\r\n",
    "# iloc is used in locating indexes. The range includes the lower bound and excludes the upper bound\r\n",
    "\r\n",
    "dataset = pd.read_csv('data\\Data.csv')\r\n",
    "x = dataset.iloc[:, :-1].values # : means the range and -1 means less the last column\r\n",
    "y = dataset.iloc[:,-1].values\r\n",
    "print(dataset)\r\n",
    "print(x)\r\n",
    "# print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Handling missing data\r\n",
    "# We can replace missing values with the mean, median or most used variable\r\n",
    "# Transform returns the columns with the replacement done. To update the replacement, we assign transform to x \r\n",
    "\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "\r\n",
    "imputer = SimpleImputer(missing_values=np.nan,strategy='mean') # replace all missing values represented by na with the mean\r\n",
    "imputer.fit(x[:, 1:3]) # Fit expects only columns with numerical values\r\n",
    "x[:, 1:3] = imputer.transform(x[:, 1:3]) # Returns the new updated version of x\r\n",
    "print(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Encoding the independent variables\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "\r\n",
    "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])],remainder='passthrough') # Pass through will not encode the other columns\r\n",
    "x = np.array(ct.fit_transform(x))\r\n",
    "print(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Encoding the dependent variable\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "le = LabelEncoder()\r\n",
    "y = le.fit_transform(y)\r\n",
    "print(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Splitting data into Training and Test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=1)\n",
    "\n",
    "print(x_train)\n",
    "print(x_test)\n",
    "print(y_train)\n",
    "print(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Feature scaling should be applied after the split\n",
    "# Standardisation and Normalisation\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "# We do not feature scale dummy variables\n",
    "x_train[:, 3:] = sc.fit_transform(x_train[:, 3:])   # Exclude dummy colums thus take from third index\n",
    "x_test[:, 3:] = sc.transform(x_test[:, 3:]) # We need same scaler thus we apply the transform method\n",
    "print(x_train)\n",
    "print(x_test)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}